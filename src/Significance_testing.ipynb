{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "from bert_score import score\n",
    "\n",
    "def compute_bert(folder_path, convert):\n",
    "    # Load all JSON files into a list of dataframes\n",
    "    dataframes = []\n",
    "    for file_name in os.listdir(folder_path):\n",
    "        if file_name.endswith(\".json\"):\n",
    "            file_path = os.path.join(folder_path, file_name)\n",
    "            try:\n",
    "                with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                    data = json.load(f)\n",
    "                df_temp = pd.json_normalize(data)\n",
    "                dataframes.append(df_temp)\n",
    "            except Exception as e:\n",
    "                print(f\"Error reading {file_path}: {e}\")\n",
    "    \n",
    "    if not dataframes:\n",
    "        raise ValueError(\"No valid JSON files found in the specified folder.\")\n",
    "    \n",
    "    df = pd.concat(dataframes, ignore_index=True)\n",
    "    if convert:\n",
    "        df[\"response\"] = df[\"output\"]\n",
    "        df[\"output\"] = df[\"label\"]\n",
    "    \n",
    "    # Define a helper function to compute BERTScore\n",
    "    def BERTScore(predictions, references, model=\"ar/QA\", device=\"\"):\n",
    "        return score(\n",
    "            cands=predictions,\n",
    "            refs=references,\n",
    "            batch_size=32,\n",
    "            model_type=model,\n",
    "            device=device,\n",
    "            num_layers=12,\n",
    "        )\n",
    "    \n",
    "    # Select the model from the map\n",
    "    model = \"bert-base-multilingual-uncased\"\n",
    "    \n",
    "    # Extract predictions and references from the dataframe\n",
    "    predictions = df[\"response\"].to_list()\n",
    "    references = df[\"output\"].to_list()\n",
    "    \n",
    "    # Compute BERTScore\n",
    "    precision, recall, f1 = BERTScore(predictions, references, model=model, device=3)\n",
    "    \n",
    "    # Format and return the mean f1 score\n",
    "    precision_ = f\"{precision.mean():.4f}\"  # sum(scores[\"precision\"]) / len(scores[\"precision\"])\n",
    "    recall_ = f\"{recall.mean():.4f}\"  # sum(scores[\"recall\"]) / len(scores[\"recall\"])\n",
    "    f1_ = f\"{f1.mean():.4f}\"  # sum(scores[\"f1\"]) / len(scores[\"f1\"])\n",
    "\n",
    "    print(f\"precision: {precision_}\\nrecall: {recall_}\\nf1: {f1_}\")\n",
    "    df[\"f1\"] = f1\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision: 0.7649\n",
      "recall: 0.7278\n",
      "f1: 0.7401\n"
     ]
    }
   ],
   "source": [
    "ft_llama = compute_bert(folder_path = \"/export/home/mohamedbayan/Bayan/LJP/Arabic-LJP/results/llama-3.2-ft\",convert = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision: 0.5262\n",
      "recall: 0.5690\n",
      "f1: 0.5439\n"
     ]
    }
   ],
   "source": [
    "base_llama = compute_bert(folder_path = \"/export/home/mohamedbayan/Bayan/LJP/results/Llama-3.2-3b-instruct/LJP\", convert = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Statistical Significance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Llama-3.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from scipy.stats import wilcoxon\n",
    "\n",
    "def compare_systems(base_df, tuned_df):\n",
    "    # Group by \"Instruction\" and calculate the mean F1 scores for both systems\n",
    "    old_system = base_df.groupby(\"Instruction\", as_index=False)[\"f1\"].mean()\n",
    "    new_system = tuned_df.groupby(\"Instruction\", as_index=False)[\"f1\"].mean()\n",
    "\n",
    "    # Merge the data on \"Instruction\"\n",
    "    merged = pd.merge(old_system,new_system, on=\"Instruction\", how=\"outer\")\n",
    "    print(merged)\n",
    "    # Drop rows with NaN values in F1 scores\n",
    "    merged = merged.dropna(subset=[\"f1_x\", \"f1_y\"])\n",
    "\n",
    "    # Extract old and new system scores\n",
    "    old_system_scores = merged[\"f1_x\"]\n",
    "    new_system_scores = merged[\"f1_y\"]\n",
    "\n",
    "    # Apply Wilcoxon signed-rank test\n",
    "    wilcoxon_statistic, wilcoxon_p_value = wilcoxon(new_system_scores, old_system_scores, alternative='greater')\n",
    "\n",
    "    # Define significance level\n",
    "    alpha = 0.05\n",
    "\n",
    "    # Print results\n",
    "    print(f\"Wilcoxon test statistic: {wilcoxon_statistic}\")\n",
    "    print(f\"P-value: {wilcoxon_p_value}\")\n",
    "    if wilcoxon_p_value < alpha:\n",
    "        print(f\"The result is statistically significant (p-value = {wilcoxon_p_value:.4f}).\")\n",
    "    else:\n",
    "        print(f\"The result is not statistically significant (p-value = {wilcoxon_p_value:.4f}).\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wilcoxon test statistic: 2850.0\n",
      "P-value: 2.6401864586600792e-14\n",
      "The result is statistically significant (p-value = 0.0000).\n"
     ]
    }
   ],
   "source": [
    "compare_systems(base_llama, ft_llama)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Llama3.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision: 0.7802\n",
      "recall: 0.7416\n",
      "f1: 0.7550\n",
      "precision: 0.5481\n",
      "recall: 0.6189\n",
      "f1: 0.5795\n"
     ]
    }
   ],
   "source": [
    "ft_llama = compute_bert(folder_path = \"/export/home/mohamedbayan/Bayan/LJP/Arabic-LJP/results/llama-3.1-ft\",convert = False)\n",
    "base_llama = compute_bert(folder_path = \"/export/home/mohamedbayan/Bayan/LJP/results/Meta-Llama-3.1-8B-Instruct/LJP\", convert = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                          Instruction      f1_x      f1_y\n",
      "0   إذا كانت هذه هي الأسباب وهذه هي الوقائع، ما نص...  0.575977  0.758273\n",
      "1   ابدأ بتحليل الأسباب، ثم اكتب نص الحكم بناءً عل...  0.582247  0.775244\n",
      "2   استخدم الأسباب لتحليل الوقائع وصياغة نص الحكم ...  0.588537  0.778784\n",
      "3   استخدم الأسباب لتحليل الوقائع وصياغة نص الحكم ...  0.568490  0.733270\n",
      "4   استخدم التحليل المنطقي والقانوني للأسباب لصياغ...  0.574055  0.753575\n",
      "..                                                ...       ...       ...\n",
      "70  ما هو نص الحكم الذي يمكن استنتاجه من الوقائع و...  0.581248  0.777551\n",
      "71  ما هو نص الحكم العادل الذي يجب إصداره وفقًا لل...  0.605633  0.744589\n",
      "72             ما هو نص الحكم المتوقع من هذه الوقائع؟  0.571130  0.696606\n",
      "73  ما هو نص الحكم المناسب الذي يمكن استنباطه من ا...  0.562017  0.747094\n",
      "74  ولد نص الحكم النهائي باستخدام التحليل القانوني...  0.610523  0.801367\n",
      "\n",
      "[75 rows x 3 columns]\n",
      "Wilcoxon test statistic: 2850.0\n",
      "P-value: 2.6401864586600792e-14\n",
      "The result is statistically significant (p-value = 0.0000).\n"
     ]
    }
   ],
   "source": [
    "compare_systems(base_llama, ft_llama)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Both Llama version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision: 0.7802\n",
      "recall: 0.7416\n",
      "f1: 0.7550\n",
      "precision: 0.7649\n",
      "recall: 0.7278\n",
      "f1: 0.7401\n"
     ]
    }
   ],
   "source": [
    "large_llama = compute_bert(folder_path = \"/export/home/mohamedbayan/Bayan/LJP/Arabic-LJP/results/llama-3.1-ft\",convert = False)\n",
    "small_llama = compute_bert(folder_path = \"/export/home/mohamedbayan/Bayan/LJP/Arabic-LJP/results/llama-3.2-ft\", convert = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wilcoxon test statistic: 2262.0\n",
      "P-value: 4.9390190229004445e-06\n",
      "The result is statistically significant (p-value = 0.0000).\n"
     ]
    }
   ],
   "source": [
    "compare_systems(small_llama, large_llama)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
